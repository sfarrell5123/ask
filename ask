#!/usr/bin/env /home/notes/ask/base/bin/python

import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Optional
import platform
from datetime import datetime
import re
import ast
import argparse
import io
import time
from openai import OpenAI
import traceback
import json_repair

# Optional Tavily import
try:
    from tavily import TavilyClient
    TAVILY_AVAILABLE = True
except ImportError:
    TAVILY_AVAILABLE = False

json_decoder = json.JSONDecoder()


def parse_tool_call_arguments(raw: Optional[str]) -> Dict:
    """Best-effort parser for tool call arguments that may contain extra text.
    Uses json_repair to fix malformed JSON from LLMs."""
    if not raw:
        return {}

    try:
        # json_repair.loads handles truncated, malformed, and dirty JSON
        return json_repair.loads(raw.strip())
    except Exception as e:
        preview = raw[:200].replace("\n", " ")
        print(f"Warning: json_repair failed: {e} - input: {preview}...", file=sys.stderr)
        return {}


class HistoryManager:
    def __init__(self):
        self.cache_dir = Path(os.path.expanduser("~/.cache/ask"))
        self.history_file = self.cache_dir / "history.json"
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def append_message(self, message: Dict) -> None:
        """Append a single message to history in native OpenAI format"""
        if not message:
            return

        history = self._load_history()
        history.append(message)
        self._save_history(history)

    def get_all_messages(self) -> List[Dict]:
        """Get all messages from history"""
        return self._load_history()

    def clear_history(self) -> None:
        """Clear all history"""
        self.history_file.unlink(missing_ok=True)

    def _load_history(self) -> List[Dict]:
        """Load history from disk - returns array of messages in OpenAI format"""
        if not self.history_file.exists():
            return []

        try:
            with open(self.history_file) as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return []

    def _save_history(self, history: List[Dict]) -> None:
        """Save history to disk - saves array of messages"""
        try:
            with open(self.history_file, "w") as f:
                json.dump(history, f, indent=2)
        except IOError:
            pass


# Singleton instance for easy access
history_manager = HistoryManager()


def read_env_file() -> Dict[str, str]:
    env_path = os.path.expanduser("~/.env")
    env_vars = {}
    if os.path.exists(env_path):
        with open(env_path) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    key, value = line.split("=", 1)
                    env_vars[key.strip()] = value.strip().strip("\"'")

    # Set defaults if not present
    if "GROQ_SMART_MODEL" not in env_vars:
        env_vars["GROQ_SMART_MODEL"] = "llama-3.3-70b-versatile"
    if "GROQ_FAST_MODEL" not in env_vars:
        env_vars["GROQ_FAST_MODEL"] = "llama-3.1-8b-instant"
    if "USE_DEEPSEEK_FOR_SMART" not in env_vars:
        env_vars["USE_DEEPSEEK_FOR_SMART"] = "true"

    return env_vars


def get_file_content(filepath: str) -> tuple[str, bool]:
    """Returns file content and whether it was truncated"""
    try:
        path = Path(filepath)
        if not path.exists():
            return None, False

        # Read full file content regardless of size
        with open(filepath, "r") as f:
            return f.read(), False
    except Exception:
        return None, False


def extract_json(response: str) -> Optional[Dict]:
    """Extract JSON from response - handles both code blocks and raw JSON.
    Uses json_repair to fix malformed JSON from LLMs."""

    if "</think>" in response:
        response = response.split("</think>", 1)[1].strip()
    if "</thinking>" in response:
        response = response.split("</thinking>", 1)[1].strip()

    # Try code block first
    response = response.replace("```tool_calls", "```json")
    match = re.search(r"```json\n(.*?)\n```", response, re.DOTALL)
    if match:
        try:
            return json_repair.loads(match.group(1).strip())
        except Exception:
            pass

    # Try raw JSON (no code block) - find first { and repair from there
    if '{' in response:
        start = response.find('{')
        if start != -1:
            try:
                return json_repair.loads(response[start:])
            except Exception:
                pass

    return None


def call_llm_simple(messages: List[Dict], model=None, extra_body_params: Dict = None) -> str:
    """Simple LLM call without tools - for text-only tasks like summarization"""

    selected_model = model or "openai/gpt-5-mini"

    # Create OpenRouter client
    openrouter_client = OpenAI(
        api_key=os.environ["OPENROUTER_API_KEY"],
        base_url="https://openrouter.ai/api/v1",
    )

    # Build extra_body if params provided
    extra_body = extra_body_params if extra_body_params else None

    # Retry logic for transient errors
    max_retries = 3
    retry_delay = 3

    for attempt in range(max_retries):
        try:
            response = openrouter_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://ask.com",
                    "X-Title": "ask",
                },
                model=selected_model,
                messages=messages,
                temperature=0,
                max_tokens=4096,
                # NO tools parameter - this is the key difference
                extra_body=extra_body,
            )
            break  # Success, exit retry loop
        except Exception as e:
            error_str = str(e).lower()
            # Check if it's a transient error
            is_transient = (
                "error code: 400" in error_str or
                "unknown error in the model inference server" in error_str or
                "error code: 500" in error_str or
                "error code: 502" in error_str or
                "error code: 503" in error_str or
                "timeout" in error_str or
                "connection" in error_str
            )

            if is_transient and attempt < max_retries - 1:
                wait_time = retry_delay * (attempt + 1)
                print(f"\033[38;5;208m⚠ Transient error: {str(e)[:80]}\033[0m", file=sys.stderr)
                print(f"\033[38;5;208m⏳ Waiting {wait_time} seconds before retry {attempt + 2}/{max_retries}...\033[0m", file=sys.stderr)
                time.sleep(wait_time)
            else:
                raise

    # Simple response extraction
    if response.usage.completion_tokens == 0:
        return ""

    content = response.choices[0].message.content.strip()

    # Remove thinking tags if present
    for tag in ("</think>", "</thinking>"):
        if tag in content:
            content = content.split(tag, 1)[1].strip()

    return content


def call_llm(messages: List[Dict], model=None, extra_body_params: Dict = None, base_url: str = None, use_native_tools: bool = True) -> str:
    """Unified function to call LLM with function calling via OpenRouter or local Ollama"""
    tools = [
        {
            "type": "function",
            "function": {
                "name": "create_plan",
                "description": "Create a step-by-step plan for complex or multi-step tasks. Use when the task requires multiple operations, coordination, or when you need user approval before proceeding. Include validation/testing steps where appropriate.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "reasoning": {
                            "type": "string",
                            "description": "Explanation of why this plan is needed",
                        },
                        "steps": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "The steps to execute the plan",
                        },
                    },
                    "required": ["reasoning", "steps"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "write_to_file",
                "description": "Write or overwrite content to a file. Creates the file if it doesn't exist. Use this to create new files or completely replace existing file contents.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "reasoning": {
                            "type": "string",
                            "description": "Explanation of why this action is needed",
                        },
                        "filename": {
                            "type": "string",
                            "description": "The path to the file to write",
                        },
                        "content": {
                            "type": "string",
                            "description": "The content to write to the file",
                        },
                    },
                    "required": ["reasoning", "filename", "content"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "execute_command",
                "description": "Execute a shell command and return its output. Use for system operations, file inspection (cat, ls, grep), running programs, checking status, etc. Avoid interactive commands (vim, nano). Use -y flags to avoid prompts (e.g., apt-get -y).",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "reasoning": {
                            "type": "string",
                            "description": "Explanation of why this command is needed",
                        },
                        "command": {
                            "type": "string",
                            "description": "The shell command to execute",
                        },
                        "params": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Optional parameters for the command",
                        },
                    },
                    "required": ["reasoning", "command"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "replace_line",
                "description": "Replace a specific line in an existing file. Use execute_command with 'cat -n filename' first to see line numbers and identify which line to replace. Line numbers are 1-based.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "reasoning": {
                            "type": "string",
                            "description": "Explanation of why this line needs to be replaced",
                        },
                        "filename": {
                            "type": "string",
                            "description": "The path to the file to modify",
                        },
                        "linenumber": {
                            "type": "integer",
                            "description": "The line number to replace (1-based)",
                        },
                        "content": {
                            "type": "string",
                            "description": "The new content for the line",
                        },
                    },
                    "required": ["reasoning", "filename", "linenumber", "content"],
                },
            },
        },
        {
            "type": "function",
            "function": {
                "name": "insert_before_line",
                "description": "Insert new content before a specific line in an existing file. Use execute_command with 'cat -n filename' first to see line numbers. Line numbers are 1-based.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "reasoning": {
                            "type": "string",
                            "description": "Explanation of why this insertion is needed",
                        },
                        "filename": {
                            "type": "string",
                            "description": "The path to the file to modify",
                        },
                        "linenumber": {
                            "type": "integer",
                            "description": "The line number to insert before (1-based)",
                        },
                        "content": {
                            "type": "string",
                            "description": "The content to insert",
                        },
                    },
                    "required": ["reasoning", "filename", "linenumber", "content"],
                },
            },
        },
    ]

    # Add Tavily tools if API key is available
    if TAVILY_AVAILABLE and os.environ.get("TAVILY_API_KEY"):
        tools.extend([
            {
                "type": "function",
                "function": {
                    "name": "web_search",
                    "description": "Search the web for current information using Tavily. Use when you need up-to-date information, documentation, or to research topics. Returns an answer summary plus detailed search results.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "reasoning": {
                                "type": "string",
                                "description": "Explanation of why this search is needed",
                            },
                            "query": {
                                "type": "string",
                                "description": "The search query to execute",
                            },
                        },
                        "required": ["reasoning", "query"],
                    },
                },
            },
            {
                "type": "function",
                "function": {
                    "name": "read_web_url",
                    "description": "Extract and read content from specific web URLs using Tavily. Use when you have exact URLs to read (from search results or user-provided). Returns the full text content from the pages.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "reasoning": {
                                "type": "string",
                                "description": "Explanation of why these URLs need to be read",
                            },
                            "urls": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "List of URLs to extract content from",
                            },
                        },
                        "required": ["reasoning", "urls"],
                    },
                },
            },
        ])

    # Model selection logic
    selected_model = model or "openai/gpt-5-mini"

    # Determine base URL and API key
    effective_base_url = base_url or "https://openrouter.ai/api/v1"
    is_ollama = base_url and "localhost" in base_url

    # Create client (Ollama doesn't need real API key)
    openrouter_client = OpenAI(
        api_key="ollama" if is_ollama else os.environ["OPENROUTER_API_KEY"],
        base_url=effective_base_url,
    )

    # Merge extra_body parameters
    extra_body = {}

    # Optional OpenRouter provider routing for DeepSeek v3
    if "deepseek" in selected_model.lower() and "v3" in selected_model.lower():
        provider_config = {"provider": {"order": ["DeepSeek", "DeepInfra"]}}
        extra_body.update(provider_config)

    # Merge caller-provided extra_body params
    if extra_body_params:
        extra_body.update(extra_body_params)

    # Retry logic for transient errors
    max_retries = 3
    retry_delay = 3

    for attempt in range(max_retries):
        try:
            # Build request kwargs
            request_kwargs = {
                "model": selected_model,
                "messages": messages,
                "temperature": 0,
                "max_tokens": 4096,
            }
            # Only use native tool calling if enabled and not Ollama
            # (Ollama models often generate malformed JSON for tool calls)
            if use_native_tools and not is_ollama:
                request_kwargs["tools"] = tools
                request_kwargs["tool_choice"] = "auto"
            # Add OpenRouter-specific headers only for non-Ollama
            if not is_ollama:
                request_kwargs["extra_headers"] = {
                    "HTTP-Referer": "https://ask.com",
                    "X-Title": "ask",
                }
                if extra_body:
                    request_kwargs["extra_body"] = extra_body

            response = openrouter_client.chat.completions.create(**request_kwargs)
            break  # Success, exit retry loop
        except Exception as e:
            error_str = str(e).lower()
            # Check if it's a transient error
            is_transient = (
                "error code: 400" in error_str or
                "unknown error in the model inference server" in error_str or
                "error code: 500" in error_str or
                "error code: 502" in error_str or
                "error code: 503" in error_str or
                "timeout" in error_str or
                "connection" in error_str
            )

            if is_transient and attempt < max_retries - 1:
                # Orange color for transient error warning
                wait_time = retry_delay * (attempt + 1)  # Exponential backoff
                # Show full error for Ollama (local debugging), truncate for OpenRouter
                error_display = str(e) if is_ollama else str(e)[:80]
                print(f"\033[38;5;208m⚠ Transient error: {error_display}\033[0m", file=sys.stderr)
                print(f"\033[38;5;208m⏳ Waiting {wait_time} seconds before retry {attempt + 2}/{max_retries}...\033[0m", file=sys.stderr)
                time.sleep(wait_time)
            else:
                # Not transient or out of retries, re-raise
                raise

    try:

        # Gracefully handle an empty assistant reply
        if response.usage.completion_tokens == 0:
            return ""

        # print(response)
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # Process tool_calls (normalize parameter names for Ollama compatibility)
        if tool_calls:
            # Store the assistant message with tool calls using model_dump()
            assistant_msg = response_message.model_dump()
            messages.append(assistant_msg)
            history_manager.append_message(assistant_msg)

            # Then process each tool call and append responses
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                function_args = parse_tool_call_arguments(tool_call.function.arguments)

                # Normalize parameter names (Ollama models may use different names)
                if "cmd" in function_args and "command" not in function_args:
                    function_args["command"] = function_args.pop("cmd")
                if "args" in function_args and "params" not in function_args:
                    function_args["params"] = function_args.pop("args")

                if function_name == "create_plan":
                    print(format_tool_feedback("create_plan", function_args))
                    result = create_plan(**function_args)
                    # Add the plan steps to messages for follow-up
                    # newMessage = {
                    #     "role": "tool",
                    #     "tool_call_id": tool_call.id,
                    #     "name": function_name,
                    #     "content": json.dumps(result),
                    # }
                    # messages.append(newMessage)
                    # Return early to allow operator review
                    # return (
                    #     "Plan created. Please review and provide feedback to proceed."
                    # )
                elif function_name == "write_to_file":
                    # Show feedback before executing
                    print(format_tool_feedback("write_to_file", function_args))
                    result = write_to_file(**function_args)
                elif function_name == "replace_line":
                    print(format_tool_feedback("replace_line", function_args))
                    result = replace_line(**function_args)
                elif function_name == "insert_before_line":
                    print(format_tool_feedback("insert_before_line", function_args))
                    result = insert_before_line(**function_args)
                elif function_name == "execute_command":
                    # Show feedback before executing
                    print(format_tool_feedback("execute_command", function_args))
                    result = execute_command(**function_args)
                elif function_name == "web_search":
                    print(format_tool_feedback("web_search", function_args))
                    result = web_search(**function_args)
                elif function_name == "read_web_url":
                    print(format_tool_feedback("read_web_url", function_args))
                    result = read_web_url(**function_args)
                else:
                    result = {"status": "error", "message": "Unknown function"}

                # Append the tool response
                tool_response = {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": function_name,
                    "content": json.dumps(result),
                }
                messages.append(tool_response)
                history_manager.append_message(tool_response)

            # After all tool calls processed, get final response
            return call_llm(messages=messages, model=selected_model, base_url=base_url, use_native_tools=use_native_tools)

        # DeepSeek v3 sometimes returns JSON‑formatted tool calls in plain text.
        # Detect a ```json``` block, execute any tool_calls_requested, then recurse.
        json_payload = extract_json(response_message.content.strip())
        if json_payload and json_payload.get("tool_calls_requested"):
            for tool in json_payload["tool_calls_requested"]:
                tool_type = tool.get("tool")
                # Dispatch by tool type
                if tool_type == "execute_command":
                    print(format_tool_feedback("execute_command", tool))
                    cmd = tool.get("command")
                    if not cmd:
                        continue
                    result = execute_command(
                        reasoning=tool.get("reasoning", ""),
                        command=cmd,
                        params=tool.get("params", [])
                    )
                elif tool_type == "create_plan":
                    print(format_tool_feedback("create_plan", tool))
                    result = create_plan(
                        reasoning=tool.get("reasoning", ""),
                        steps=tool["steps"]
                    )
                elif tool_type == "write_to_file":
                    print(format_tool_feedback("write_to_file", tool))
                    result = write_to_file(
                        reasoning=tool.get("reasoning", ""),
                        filename=tool["filename"],
                        content=tool["content"]
                    )
                elif tool_type in {"modify_file", "replace_line", "insert_before_line"}:
                    op = tool.get("operation", "replace")
                    if op == "insert":
                        print(format_tool_feedback("insert_before_line", tool))
                        result = insert_before_line(
                            reasoning=tool.get("reasoning", ""),
                            filename=tool["filename"],
                            linenumber=tool["linenumber"],
                            content=tool["content"]
                        )
                    else:
                        print(format_tool_feedback("replace_line", tool))
                        result = replace_line(
                            reasoning=tool.get("reasoning", ""),
                            filename=tool["filename"],
                            linenumber=tool["linenumber"],
                            content=tool["content"]
                        )
                elif tool_type == "text_response":
                    if tool.get("reasoning"):
                        print(f"\\033[38;5;153m{tool['reasoning']}\\033[0m")
                    return tool["text"]
                else:
                    print(f"\\033[31mUnknown tool type: {tool_type}\\033[0m")

                # Record tool outcome
                messages.append({
                    "role": "tool",
                    "name": tool_type,
                    "content": json.dumps(result)
                })
                history_manager.append_message(messages[-1])

            # After executing all tools, recurse to get the assistant's follow‑up
            return call_llm(messages=messages, model=selected_model, base_url=base_url, use_native_tools=use_native_tools)

        # Check for bash code block pattern
        content = response_message.content.strip()
        lines = content.strip().splitlines()
        if (
            content.startswith("```bash")
            and len(lines) >= 3
            and lines[2].strip().endswith("```")
        ):
            # Extract the command line
            command_line = content.splitlines()[1].strip()
            # Split into command and args
            parts = command_line.split()
            command = parts[0]
            params = parts[1:] if len(parts) > 1 else None
            # Execute directly with feedback
            print(
                format_tool_feedback(
                    "execute_command", {"command": command, "params": params}
                )
            )
            result = execute_command(command=command, params=params)

            # Add the command and result to messages similar to tool calls
            assistant_message = {"role": "assistant", "content": content}
            messages.append(assistant_message)
            history_manager.append_message(assistant_message)

            command_result_message = {
                "role": "user",
                "content": f"Command executed: {command_line}\nResult: {json.dumps(result)}",
            }
            messages.append(command_result_message)
            history_manager.append_message(command_result_message)

            print("asking a second time for bash command")
            # Get the final response after command execution using recursive call
            return call_llm(
                messages=messages,
                model=selected_model,
                base_url=base_url,
                use_native_tools=use_native_tools,
            )

        # Look for code‑block tool calls (python/json/code/xml)
        code_block_tools = extract_tool_calls_from_code_blocks(content)
        if code_block_tools:
            for tool in code_block_tools:
                tool_type = tool.get("tool")
                if tool_type == "execute_command":
                    print(format_tool_feedback("execute_command", tool))
                    result = execute_command(
                        reasoning=tool.get("reasoning", ""),
                        command=tool["command"],
                        params=tool.get("params", []),
                    )
                elif tool_type == "create_plan":
                    print(format_tool_feedback("create_plan", tool))
                    result = create_plan(
                        reasoning=tool.get("reasoning", ""),
                        steps=tool["steps"],
                    )
                elif tool_type == "write_to_file":
                    print(format_tool_feedback("write_to_file", tool))
                    result = write_to_file(
                        reasoning=tool.get("reasoning", ""),
                        filename=tool["filename"],
                        content=tool["content"],
                    )
                elif tool_type == "replace_line":
                    print(format_tool_feedback("replace_line", tool))
                    result = replace_line(
                        reasoning=tool.get("reasoning", ""),
                        filename=tool["filename"],
                        linenumber=tool["linenumber"],
                        content=tool["content"],
                    )
                elif tool_type == "insert_before_line":
                    print(format_tool_feedback("insert_before_line", tool))
                    result = insert_before_line(
                        reasoning=tool.get("reasoning", ""),
                        filename=tool["filename"],
                        linenumber=tool["linenumber"],
                        content=tool["content"],
                    )
                else:
                    continue  # Unknown tool, skip

                # Record tool response
                messages.append(
                    {
                        "role": "tool",
                        "name": tool_type,
                        "content": json.dumps(result),
                    }
                )
                history_manager.append_message(messages[-1])

            # Recurse to let the assistant generate the follow‑up answer
            return call_llm(messages=messages, model=selected_model, base_url=base_url, use_native_tools=use_native_tools)

        # Try XML format tool calls
        xml_tools = extract_xml_tool_calls(content)
        if xml_tools:
            for tool in xml_tools:
                tool_type = tool.get("tool")
                if tool_type == "execute_command":
                    print(format_tool_feedback("execute_command", tool))
                    result = execute_command(
                        reasoning=tool.get("reasoning", ""),
                        command=tool["command"],
                        params=tool.get("params", []) if isinstance(tool.get("params"), list) else []
                    )
                elif tool_type == "create_plan":
                    print(format_tool_feedback("create_plan", tool))
                    steps = tool.get("steps", [])
                    if isinstance(steps, str):
                        # If steps is a string, try to parse as JSON array
                        try:
                            steps = json.loads(steps)
                        except:
                            steps = [steps]
                    result = create_plan(
                        reasoning=tool.get("reasoning", ""),
                        steps=steps
                    )
                elif tool_type == "write_to_file":
                    print(format_tool_feedback("write_to_file", tool))
                    result = write_to_file(
                        reasoning=tool.get("reasoning", ""),
                        filename=tool["filename"],
                        content=tool["content"]
                    )
                elif tool_type == "replace_line":
                    print(format_tool_feedback("replace_line", tool))
                    result = replace_line(
                        reasoning=tool.get("reasoning", ""),
                        filename=tool["filename"],
                        linenumber=int(tool["linenumber"]),
                        content=tool["content"]
                    )
                elif tool_type == "insert_before_line":
                    print(format_tool_feedback("insert_before_line", tool))
                    result = insert_before_line(
                        reasoning=tool.get("reasoning", ""),
                        filename=tool["filename"],
                        linenumber=int(tool["linenumber"]),
                        content=tool["content"]
                    )
                else:
                    continue

                # Record tool response
                messages.append({
                    "role": "tool",
                    "name": tool_type,
                    "content": json.dumps(result)
                })
                history_manager.append_message(messages[-1])

            # Recurse to get final response
            return call_llm(messages=messages, model=selected_model, base_url=base_url, use_native_tools=use_native_tools)

        # Remove any thinking tags and get just the answer content
        answer = content
        for tag in ("</think>", "</thinking>"):
            if tag in answer:
                answer = answer.split(tag, 1)[1].strip()

        # If the assistant returned nothing, summarise the last tool output
        if answer.strip() == "":
            for msg in reversed(messages):
                if msg.get("role") == "tool":
                    tool_name = msg.get("name", "")
                    try:
                        result = json.loads(msg.get("content", "{}"))
                    except Exception:
                        break
                    if tool_name == "execute_command":
                        stdout = (result.get("stdout") or "").strip().splitlines()
                        answer = "\n".join(stdout[:20])
                    else:
                        answer = json.dumps(result, indent=2)[:400]
                    break

        return answer
    except Exception as e:
        # Print debug info - last 2 messages only to avoid flooding
        print("\n--- Debug: Last 2 messages ---", file=sys.stderr)
        for msg in messages[-2:]:
            role = msg.get("role", "?")
            content = msg.get("content", "")
            if len(content) > 500:
                content = content[:500] + "..."
            print(f"[{role}]: {content}", file=sys.stderr)
        print("--- End debug ---\n", file=sys.stderr)
        print(f"Error with primary model: {str(e)}", file=sys.stderr)
        print(traceback.format_exc(), file=sys.stderr)
        exit(1)

        # If Deepseek failed, try Groq as fallback
        if os.environ.get("USE_DEEPSEEK_FOR_SMART", "true").lower() == "true":
            try:
                response = client.chat.completions.create(
                    messages=messages,
                    model=os.environ.get("GROQ_SMART_MODEL", "llama-3.3-70b-versatile"),
                )
                return response.choices[0].message.content
            except Exception as fallback_e:
                print(f"Error with fallback model: {str(fallback_e)}", file=sys.stderr)
                raise fallback_e
        raise e


def write_to_file(*, reasoning: str = "", filename: str, content: str) -> Dict:
    """Function to write content to a file"""
    try:
        with open(filename, "w") as f:
            f.write(content)
        return {"status": "success", "message": f"Content written to {filename}"}
    except Exception as e:
        return {"status": "error", "message": str(e)}


def create_plan(*, reasoning: str = "", steps: List[str]) -> Dict:
    """Function to create a step-by-step plan"""
    return {
        "status": "success",
        "message": f"Created plan with {len(steps)} steps",
        "steps": steps,
    }


def format_tool_feedback(tool_name: str, params: Dict) -> str:
    """Format tool call feedback for user display"""
    # Print reasoning in light lavender
    reasoning = params.get("reasoning", "")
    if reasoning:
        print(f"\033[38;5;153m{reasoning}\033[0m")

    # Create a short summary of the params
    param_summary = ""
    if tool_name == "create_plan":
        steps = params.get("steps", [])
        step_summary = "\n".join(
            f"  {i+1}. {step}" for i, step in enumerate(steps[:10])
        )
        if len(steps) > 10:
            step_summary += f"\n  ... and {len(steps)-10} more steps"
        return f"\033[92m<{tool_name}>\033[0m\n{step_summary}"
    if tool_name == "write_to_file":
        filename = params.get("filename", "")
        content_preview = (
            (params.get("content", "")[:50] + "...")
            if len(params.get("content", "")) > 50
            else params.get("content", "")
        )
        param_summary = f"{filename}: {content_preview}"
    elif tool_name == "replace_line":
        filename = params.get("filename", "")
        linenumber = params.get("linenumber", 0)
        content_preview = (
            (params.get("content", "")[:50] + "...")
            if len(params.get("content", "")) > 50
            else params.get("content", "")
        )
        param_summary = f"{filename}:{linenumber} -> {content_preview}"
    elif tool_name == "insert_before_line":
        filename = params.get("filename", "")
        linenumber = params.get("linenumber", 0)
        content_preview = (
            (params.get("content", "")[:50] + "...")
            if len(params.get("content", "")) > 50
            else params.get("content", "")
        )
        param_summary = f"{filename}:{linenumber} <- {content_preview}"
    elif tool_name == "execute_command":
        command = params.get("command", "")
        params_list = params.get("params", [])
        param_summary = f"{command} {' '.join(params_list)}"
    elif tool_name == "web_search":
        query = params.get("query", "")
        param_summary = f"'{query}'"
    elif tool_name == "read_web_url":
        urls = params.get("urls", [])
        if isinstance(urls, list) and urls:
            param_summary = f"{urls[0]}" + (f" +{len(urls)-1} more" if len(urls) > 1 else "")
        else:
            param_summary = str(urls)

    # Format with bright green highlight
    return f"\033[92m<{tool_name}>\033[0m {param_summary}"


def modify_file_lines(filename: str, linenumber: int, content: str, mode: str) -> Dict:
    """Helper function to modify file lines"""
    try:
        with open(filename, "r") as f:
            lines = f.readlines()

        linenumber = linenumber - 1  # Convert to 0-based index
        if linenumber < 0 or linenumber >= len(lines):
            return {
                "status": "error",
                "message": f"Line number {linenumber+1} is out of range",
            }

        if mode == "replace":
            lines[linenumber] = content + "\n"
        elif mode == "insert":
            lines.insert(linenumber, content + "\n")

        with open(filename, "w") as f:
            f.writelines(lines)

        return {
            "status": "success",
            "message": f"File {filename} modified successfully",
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}


def replace_line(*, reasoning: str = "", filename: str, linenumber: int, content: str) -> Dict:
    """Replace a specific line in a file"""
    return modify_file_lines(filename, linenumber, content, "replace")


def insert_before_line(
    reasoning: str, filename: str, linenumber: int, content: str
) -> Dict:
    """Insert content before a specific line in a file"""
    return modify_file_lines(filename, linenumber, content, "insert")


def execute_command(*, reasoning: str = "", command, params: List[str] = None) -> Dict:
    """Function to execute a shell command with optional parameters"""
    try:
        import subprocess

        # Handle command as list (some models send ['bash', '-c', 'actual command'])
        if isinstance(command, list):
            # If it's a bash -c style list, extract the actual command
            if len(command) >= 3 and command[0] in ('bash', 'sh') and command[1] in ('-c', '-lc'):
                command = command[2]
            else:
                command = ' '.join(command)

        # If params are provided, join them with the command
        full_command = command
        if params:
            full_command = f"{command} {' '.join(params)}"

        result = subprocess.run(
            full_command, shell=True, capture_output=True, text=True
        )

        # Print the first line of output in sky blue
        if result.stdout:
            first_line = result.stdout.splitlines()[0]
            print(f"\033[96m<output>\033[0m {first_line}")

        # Return full stdout - no truncation
        return {
            "status": "success",
            "stdout": result.stdout,
            "stderr": result.stderr,
            "returncode": result.returncode,
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}


def web_search(*, reasoning: str = "", query: str) -> Dict:
    """Search the web using Tavily API"""
    if not TAVILY_AVAILABLE:
        return {"status": "error", "message": "Tavily not available - install tavily-python"}

    try:
        api_key = os.environ.get("TAVILY_API_KEY")
        if not api_key:
            return {"status": "error", "message": "TAVILY_API_KEY not found in environment"}

        client = TavilyClient(api_key)
        response = client.search(
            query=query,
            include_answer="basic",
            search_depth="advanced"
        )

        # Print answer if available
        if response.get("answer"):
            print(f"\033[96m<answer>\033[0m {response['answer'][:100]}...")

        return {
            "status": "success",
            "answer": response.get("answer", ""),
            "results": response.get("results", []),
            "query": response.get("query", query)
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}


def read_web_url(*, reasoning: str = "", urls: List[str]) -> Dict:
    """Extract content from web URLs using Tavily API"""
    if not TAVILY_AVAILABLE:
        return {"status": "error", "message": "Tavily not available - install tavily-python"}

    try:
        api_key = os.environ.get("TAVILY_API_KEY")
        if not api_key:
            return {"status": "error", "message": "TAVILY_API_KEY not found in environment"}

        client = TavilyClient(api_key)
        response = client.extract(urls=urls)

        # Print first URL extracted
        if response.get("results") and len(response["results"]) > 0:
            first_url = response["results"][0].get("url", "")
            print(f"\033[96m<extracted>\033[0m {first_url}")

        return {
            "status": "success",
            "results": response.get("results", []),
            "failed_results": response.get("failed_results", [])
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}


def extract_xml_tool_calls(text: str) -> List[Dict]:
    """
    Extract tool calls from XML format like:
    <function_calls>
      <invoke name="execute_command">
        <parameter name="reasoning">...</parameter>
        <parameter name="command">...</parameter>
      </invoke>
    </function_calls>
    """
    import re
    import xml.etree.ElementTree as ET

    tool_calls: List[Dict] = []

    # Look for <function_calls> blocks
    if "<function_calls>" not in text or "<invoke" not in text:
        return tool_calls

    try:
        # Extract the XML content
        match = re.search(r'<function_calls>(.*?)</function_calls>', text, re.DOTALL)
        if not match:
            return tool_calls

        xml_content = f"<function_calls>{match.group(1)}</function_calls>"
        root = ET.fromstring(xml_content)

        # Process each <invoke> element
        for invoke in root.findall('invoke'):
            tool_name = invoke.get('name')
            if not tool_name:
                continue

            # Map XML tool names to our function names
            tool_name_map = {
                'execute_command': 'execute_command',
                'create_plan': 'create_plan',
                'write_to_file': 'write_to_file',
                'replace_line': 'replace_line',
                'insert_before_line': 'insert_before_line',
            }

            if tool_name not in tool_name_map:
                continue

            # Extract parameters
            params = {'tool': tool_name_map[tool_name]}
            for param in invoke.findall('parameter'):
                param_name = param.get('name')
                param_value = param.text or ''
                params[param_name] = param_value

            tool_calls.append(params)
    except Exception:
        # If XML parsing fails, return empty list
        pass

    return tool_calls


def extract_tool_calls_from_code_blocks(text: str) -> List[Dict]:
    """
    Find ```python|json|code|xml blocks and pull out function‑style tool calls
    such as execute_command(...).  Returns a list of dicts ready for dispatch.
    No regular expressions are used.
    """
    import textwrap

    tool_calls: List[Dict] = []
    if "```" not in text:
        return tool_calls

    segments = text.split("```")
    # Odd indices (1,3,5,...) are the code‑block parts
    for i in range(1, len(segments), 2):
        block = segments[i]
        lines = block.splitlines()
        if not lines:
            continue
        # Remove the language identifier (first line) if present
        code_body = "\n".join(lines[1:]) if len(lines) > 1 else ""
        snippet = textwrap.dedent(code_body).strip()
        if not snippet:
            continue
        try:
            node = ast.parse(snippet).body[0]
            if isinstance(node, ast.Expr) and isinstance(node.value, ast.Call):
                call = node.value
                if isinstance(call.func, ast.Name):
                    func_name = call.func.id
                    if func_name in {
                        "execute_command",
                        "create_plan",
                        "write_to_file",
                        "replace_line",
                        "insert_before_line",
                    }:
                        # Collect keyword args first
                        kwargs = {
                            kw.arg: ast.literal_eval(kw.value)
                            for kw in call.keywords
                        }

                        # Map positional args to expected names
                        pos = [ast.literal_eval(a) for a in call.args]
                        if func_name == "execute_command":
                            if len(pos) >= 1 and "reasoning" not in kwargs:
                                kwargs["reasoning"] = pos[0]
                            if len(pos) >= 2 and "command" not in kwargs:
                                kwargs["command"] = pos[1]
                            if len(pos) >= 3 and "params" not in kwargs:
                                kwargs["params"] = pos[2]
                        elif func_name == "create_plan":
                            if len(pos) >= 1 and "reasoning" not in kwargs:
                                kwargs["reasoning"] = pos[0]
                            if len(pos) >= 2 and "steps" not in kwargs:
                                kwargs["steps"] = pos[1]
                        elif func_name in {"write_to_file", "replace_line", "insert_before_line"}:
                            # Generic positional mapping if someone omits keywords
                            param_order = {
                                "write_to_file": ["reasoning", "filename", "content"],
                                "replace_line": ["reasoning", "filename", "linenumber", "content"],
                                "insert_before_line": ["reasoning", "filename", "linenumber", "content"],
                            }[func_name]
                            for idx, val in enumerate(pos):
                                if idx < len(param_order) and param_order[idx] not in kwargs:
                                    kwargs[param_order[idx]] = val

                        tool_dict = {"tool": func_name}
                        tool_dict.update(kwargs)
                        tool_calls.append(tool_dict)
        except Exception:
            # Ignore unparsable code blocks
            continue
    return tool_calls


def format_messages_for_summary(messages: List[Dict]) -> str:
    """Format messages into compact text structure for summarization"""
    lines = []
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")

        if role == "user":
            lines.append(f"user: {content}")
        elif role == "assistant":
            # Handle tool calls
            if msg.get("tool_calls"):
                tool_names = [tc.get("function", {}).get("name", "unknown") for tc in msg["tool_calls"]]
                lines.append(f"assistant: [tool calls: {', '.join(tool_names)}]")
            else:
                lines.append(f"assistant: {content}")
        elif role == "tool":
            name = msg.get("name", "unknown")
            lines.append(f"tool ({name}): {content[:100]}...")  # Truncate tool output

    return "\n".join(lines)


def summarize_conversation(conversation_text: str) -> str:
    """Use fast LLM to create compact summary of conversation"""
    summarization_prompt = """Your task: Compress conversation history to reduce token usage while preserving all important information.

OUTPUT FORMAT:
Use exactly these labels for each turn:
- user: [message content]
- assistant: [response content]  
- assistant: [tool call: function_name with parameters and reasoning]
- tool: [result: key outputs or data]

COMPRESSION RULES:
1. Maximum 300 characters PER ITEM (each user message, each assistant response, each tool call, each tool result)
2. For turns with multiple tool calls: treat EACH tool call as a separate item with its own 300-char budget
3. For tool results: Each result gets its own 300-char budget to preserve key data/outputs

4. If item is already under 300 characters:
   - Keep the core content mostly intact
   - Remove filler words, redundancy, and verbose phrasing
   - Compress language where possible without losing meaning

5. If item exceeds 300 characters:
   - Paraphrase into concise summary
   - Keep all key facts, decisions, and outcomes
   - Preserve technical details, names, numbers, and specific requests

SPECIAL HANDLING FOR TOOL CALLS:
- Include: function name, key parameters/arguments, and reasoning (if present)
- Format: [tool call: function_name(param1, param2) - reasoning]
- Use full 300-char budget to preserve command details and context
- Example: [tool call: execute_command("git log --oneline -10 --format='%h %ad %s' --date=short") - Get detailed commit info with dates]

SPECIAL HANDLING FOR TOOL RESULTS:
- keep 300 characters of actual data/output - compressed/summarized/paraphrased to preserve as much meaning as possible.
- Include key numbers, strings, error messages, or data returned
- Use full 300-char budget to keep meaningful results
- Example: [result: 12 total commits. Latest: 81b66a6 (Nov 10) refactor OpenAI format, a080276 (Nov 9) Tavily search, b0f6ea4 (Nov 9) API key fixes, etc.]

REQUIREMENTS:
- Maintain the exact sequence of all turns and tool calls
- Never skip or truncate turns
- Keep factual accuracy - don't invent or change meaning
- Preserve context needed for future turns to make sense
- Each tool call = separate item with 300-char budget
- Each tool result = separate item with 300-char budget

IMPORTANT:
- 300 character budget per turn/tool call/tool result

EXAMPLE:
Input assistant turn with tool call (2 items):
"assistant: [tool_calls with command]"
"tool: [long result with git log output]"

Output (2 items, each up to 300 chars):
"assistant: [tool call: execute_command('git log --oneline -10') - Check recent commits and their messages]"
"tool: [result: 10 commits shown: 81b66a6 refactor OpenAI format, a080276 add Tavily search, b0f6ea4 fix API key check, d7e1b65 load env vars early, 644b2cc show API status in help]

BAD:
tool (create_plan): {"status": "success", "message": "Created plan with 3 steps", "steps": ["Flush the swap by running '...  (don't snip the plan, we lost important information here)
assistant: [tool calls: execute_command, execute_command, execute_command]"  (execute what command ? )

Format output as plain text with the structure above. Do not add any explanations or meta-commentary."""

    try:
        print(f"Calling LLM to summarize {len(conversation_text)} characters...")
        # NOTE: keep using gpt-5-mini here; avoid reverting to legacy 4o-mini.
        # Keeping reasoning effort low (per OpenRouter reasoning token docs) to reduce latency.
        summary = call_llm_simple(
            messages=[
                {"role": "system", "content": summarization_prompt},
                {"role": "user", "content": f"Compact this conversation:\n\n{conversation_text}"},
            ],
            model="openai/gpt-5-mini",
            extra_body_params={"reasoning": {"effort": "low"}}
        )
        print(f"LLM returned summary of {len(summary)} characters")
        if not summary or not summary.strip():
            print("Warning: LLM returned empty summary, using original text", file=sys.stderr)
            return conversation_text
        return summary
    except Exception as e:
        print(f"Warning: Failed to compact history: {str(e)}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return conversation_text  # Return original if summarization fails


def compact_history() -> str:
    """Load all history, summarize it compactly, save as single entry"""
    # Load all history
    all_messages = history_manager.get_all_messages()

    if not all_messages:
        return "No history to compact."

    # Build conversation text from messages
    conversation_text = format_messages_for_summary(all_messages)

    # Call fast LLM to summarize
    print(f"Compacting {len(all_messages)} messages...")
    summary = summarize_conversation(conversation_text)

    # Clear history and save single compact summary message
    history_manager.clear_history()
    history_manager.append_message({"role": "assistant", "content": f"[Compact summary of previous conversation]\n{summary}"})

    return summary


def show_history_summary() -> None:
    """Display summary of history without modifying it"""

    # Check file size and suggest compacting if needed
    if history_manager.history_file.exists():
        file_size = history_manager.history_file.stat().st_size
        if file_size > 10000:  # 10KB threshold
            print(f"* suggest compacting: History file is {file_size:,} bytes (>{10000:,} bytes)")
            print()

    # Load all history
    all_messages = history_manager.get_all_messages()

    if not all_messages:
        print("No history to summarize.")
        return

    # Build conversation text and summarize (reuse existing functions)
    conversation_text = format_messages_for_summary(all_messages)
    print(f"Summarizing {len(all_messages)} messages...")
    summary = summarize_conversation(conversation_text)

    # Display summary
    print()
    print("=" * 60)
    print("HISTORY SUMMARY (read-only)")
    print("=" * 60)
    print(summary)
    print("=" * 60)
    print()
    print(f"Summary generated from {len(all_messages)} messages ({len(summary)} characters)")


def handle_file_operation(operation_type: str, data: Dict) -> str:
    """Handle file operations from JSON data"""
    try:
        if operation_type == "write_to_file":
            result = write_to_file(
                reasoning=data.get("reasoning", ""),
                filename=data["filename"],
                content=data["content"],
            )
        elif operation_type == "replace_line":
            result = replace_line(
                reasoning=data.get("reasoning", ""),
                filename=data["filename"],
                linenumber=data["linenumber"],
                content=data["content"],
            )
        elif operation_type == "insert_before_line":
            result = insert_before_line(
                reasoning=data.get("reasoning", ""),
                filename=data["filename"],
                linenumber=data["linenumber"],
                content=data["content"],
            )
        else:
            return "Unknown file operation"

        return f"{operation_type} result: {json.dumps(result)}"
    except KeyError as e:
        return f"Missing required field: {str(e)}"


def main():
    # Read environment variables FIRST (before argparse checks for --help)
    env_vars = read_env_file()
    os.environ.update(env_vars)

    # Debug environment variables if requested
    if "--debug-env" in sys.argv:
        print("=== Environment Debug ===")
        print(f"~/.env path: {os.path.expanduser('~/.env')}")
        print(f"~/.env exists: {os.path.exists(os.path.expanduser('~/.env'))}")
        print(f"\nVariables loaded from ~/.env:")
        for key in sorted(env_vars.keys()):
            if "KEY" in key or "API" in key:
                value = env_vars[key]
                masked = value[:8] + "..." if len(value) > 8 else value
                print(f"  {key} = {masked} (len={len(value)})")
        print(f"\nTAVILY_API_KEY in env_vars: {'TAVILY_API_KEY' in env_vars}")
        print(f"TAVILY_API_KEY in os.environ: {'TAVILY_API_KEY' in os.environ}")
        print(f"TAVILY_AVAILABLE: {TAVILY_AVAILABLE}")
        if 'TAVILY_API_KEY' in os.environ:
            val = os.environ['TAVILY_API_KEY']
            print(f"TAVILY_API_KEY value: {val[:8]}... (len={len(val)})")
        print("=" * 40)
        sys.exit(0)

    if len(sys.argv) < 2:
        print("Usage: ask <question> [file1] [file2] ...")
        sys.exit(1)

    # Check for clear history flag
    if "--clear" in sys.argv:
        history_manager.clear_history()
        print("History cleared.")
        sys.argv.remove("--clear")  # Remove the flag from arguments
        sys.exit(0)  # Exit after clearing history

    # Check for compact history flag
    if "--compact" in sys.argv:
        summary = compact_history()
        print("History compacted successfully!")
        print(f"\nCompact summary ({len(summary)} characters):")
        print("-" * 60)
        print(summary)
        print("-" * 60)
        sys.exit(0)  # Exit after compacting history

    # Check for summary flag
    if "--summary" in sys.argv:
        show_history_summary()
        sys.exit(0)  # Exit after showing summary

    # Check history file size and suggest compacting if needed
    if history_manager.history_file.exists():
        file_size = history_manager.history_file.stat().st_size
        if file_size > 10000:  # 10KB threshold
            # Yellow/gold warning on one line
            print(f"\033[38;5;220m⚠ History: {file_size:,} bytes (>10KB) - Consider: ask --compact\033[0m", file=sys.stderr)

    # Initialize argument parser
    # Build API key status for help text (match actual runtime checks)
    openrouter_status = "✓ Available" if "OPENROUTER_API_KEY" in os.environ else "✗ Not found"

    # More detailed Tavily status
    if TAVILY_AVAILABLE and os.environ.get("TAVILY_API_KEY"):
        tavily_status = "✓ Available"
    elif not TAVILY_AVAILABLE:
        tavily_status = "✗ Package not installed (pip install tavily-python)"
    else:
        tavily_status = "✗ API key not found"

    epilog_text = f"""
API Key Status:
  OPENROUTER_API_KEY: {openrouter_status}
  TAVILY_API_KEY:     {tavily_status} (optional - enables web_search and read_web_url tools)
"""

    parser = argparse.ArgumentParser(
        description="Ask questions and perform tasks using a language model.",
        epilog=epilog_text,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("question", nargs="+", help="The question to ask.")
    parser.add_argument(
        "files", nargs="*", help="Optional files to provide context.", default=[]
    )
    parser.add_argument(
        "--clear", action="store_true", help="Clear the chat history."
    )
    parser.add_argument(
        "--compact", action="store_true", help="Compact and summarize all chat history."
    )
    parser.add_argument(
        "--summary", action="store_true", help="Display summary of chat history without modifying it."
    )
    parser.add_argument("--fast", action="store_true", help="Use Gemini Flash model")
    parser.add_argument("--alt", action="store_true", help="Use alternative model (Kimi K2)")
    parser.add_argument("--model", type=str, help="Specify a custom model to use")
    parser.add_argument("--ollama", action="store_true", help="Use local Ollama with gpt-oss:latest")
    parser.add_argument("--no-tools", action="store_true", help="Disable native tool calling (use text-based fallback parsing)")

    args = parser.parse_args()

    # Handle clear history flag
    if args.clear:
        history_manager.clear_history()
        print("History cleared.")
        sys.exit(0)

    # Handle compact history flag
    if args.compact:
        summary = compact_history()
        print("History compacted successfully!")
        print(f"\nCompact summary ({len(summary)} characters):")
        print("-" * 60)
        print(summary)
        print("-" * 60)
        sys.exit(0)

    # Check for OPENROUTER_API_KEY
    if "OPENROUTER_API_KEY" not in os.environ:
        print("Error: OPENROUTER_API_KEY not found in environment variables")
        sys.exit(1)

    # Process arguments and build context
    files_content = []
    question = []

    # Process files
    for arg in args.files:
        file_content, truncated = get_file_content(arg)
        if file_content is not None:
            files_content.append(
                {"path": arg, "content": file_content, "truncated": truncated}
            )
            question.append(f"[File content from: {arg}]")
            if truncated:
                question.append("(First 100 lines of file:)")
            question.append(file_content)

    # Combine question parts
    question.extend(args.question)

    # Check for piped input
    if not sys.stdin.isatty():
        piped_content = sys.stdin.read().strip()
        if piped_content:
            files_content.append(
                {"path": "<stdin>", "content": piped_content, "truncated": False}
            )
            question.append("[Content from stdin:]")
            question.append(piped_content)

    # Check for stderr input
    if not sys.stderr.isatty() and getattr(sys.stderr, "readable", lambda: False)():
        try:
            stderr_content = sys.stderr.read().strip()
        except io.UnsupportedOperation:
            stderr_content = ""
        if stderr_content:
            files_content.append(
                {"path": "<stderr>", "content": stderr_content, "truncated": False}
            )
            question.append("[Content from stderr:]")
            question.append(stderr_content)

    # Build the prompt and messages
    question_str = "\n".join(question)
    system_prompt = f"""# Terminal Assistant Prompt

## Overview
You are a helpful command-line assistant providing terminal execution and answers to a user. 
Provide help and utility to the user by executing functions/commands and providing answers.
You have access to the PC via the terminal - use your tools/functions to help the user.
Your responses should be formatted appropriately for terminal display and context-aware based on the user's environment.
The user maybe be asking a question or for a task to be done.

## Environment Context
OS: {platform.system()} {platform.release()}
PWD: {os.getcwd()}
Shell: {os.environ.get("SHELL", "unknown")}

## Operating Procedures
  
### Task Execution Flow (Refined)
1. **Understand the Task**  
   - Identify what the user is asking for and assess how complex the request is.
2. **Decide on Approach**  
   - If the task is **simple**, proceed directly with solving the problem.  
   - If the task is **complex**, start by creating a plan using `create_plan`.
3. **Propose/Refine Steps**  
   - If you create a plan, present it for **user feedback** or approval before carrying it out.  
   - Adjust your approach as needed based on feedback.
4. **Execute and Validate**  
   - Implement your chosen steps one by one.  
   - Validate the results after each significant operation to ensure correctness.
5. **Iterate with Feedback**  
   - You will receive feedback on each completed task or subtask.  
   - Use this feedback to refine your next steps, as there may be multiple opportunities to adjust your approach.
6. **Deliver the Final Answer**  
   - Once you have completed all necessary actions and validations, provide a concise, clear response to the user summarizing your findings.


## Best Practices

### efficiency
* try to minimize the liekly output of commands. 
* using grep for example, if you know the like you want, cat full.txt  vs cat full.txt|grep -i title  (this is much more efficient)
* efficiency is important, helps keep things tight and focussed

### File Handling
* Use `execute_command` with `cat -n` to obtain line numbers before file modifications
* For large files, use `head` or `tail` as appropriate
* Always verify file contents after modifications

### Safety and Validation
* Provide clear reasoning for all operations
* Include explicit testing steps in plans
* Verify results after executing commands
* Maintain backup copies of modified files when appropriate
* try not to make assumptions, gather information, like `date` if the date is important to the conversation. Or get the cmd line or python to do math for you, etc

### User Interaction
* Format all responses for terminal readability. green tick for when things went well
* Wait for explicit user confirmation before executing plans
"""

    if files_content:
        system_prompt += "\nThe following files/content were provided:\n"
        for fc in files_content:
            system_prompt += f"\n- File: {fc['path']}"
            if fc["truncated"]:
                system_prompt += " (truncated to first 100 lines)"
            system_prompt += f"\nContent:\n{fc['content']}\n"

    # Prepare messages with history
    messages = []
    messages.append({"role": "system", "content": system_prompt})

    # Add all history
    history = history_manager.get_all_messages()
    messages.extend(history)

    # Add current question
    user_message = {"role": "user", "content": question_str}
    history_manager.append_message(user_message)
    messages.append(user_message)

    # print("kicking off with:", messages)
    # sys.exit(0)

    # Set model based on flags
    base_url = None  # Default to OpenRouter
    if args.ollama:
        selected_model = "gpt-oss:latest"
        base_url = "http://localhost:11434/v1"
    elif args.model:
        selected_model = args.model
    elif args.fast:
        selected_model = "google/gemini-2.5-flash-lite"
    elif args.alt:
        # Try fallback chain: grok-code-fast -> kimi-k2-thinking -> gpt-5-mini
        fallback_models = [
           # "deepseek/deepseek-v3.2",
         #   "nvidia/nemotron-3-nano-30b-a3b:free",
"xiaomi/mimo-v2-flash:free",
            "kwaipilot/kat-coder-pro:free",
            "x-ai/grok-code-fast-1",
            "moonshotai/kimi-k2-thinking",
            "openai/gpt-5-mini"
        ]
        selected_model = fallback_models[0]
    else:
        selected_model = "openai/gpt-5-mini"
    print("selected model: ", selected_model)

    # Make the API call with fallback support for --alt
    answer = None
    if args.alt:
        # Try each model in fallback chain
        for i, model in enumerate(fallback_models):
            try:
                print(f"Trying model: {model}")
                answer = call_llm(
                    messages=messages,
                    model=model,
                    use_native_tools=not args.no_tools,
                )
                print(f"✓ Success with {model}")
                break
            except Exception as e:
                error_msg = str(e)
                print(f"✗ {model} failed: {error_msg[:80]}", file=sys.stderr)
                if i < len(fallback_models) - 1:
                    print(f"  Falling back to {fallback_models[i+1]}...", file=sys.stderr)
                else:
                    print(f"Error: All fallback models failed", file=sys.stderr)
                    sys.exit(1)
    else:
        # Normal single model call
        try:
            answer = call_llm(
                messages=messages,
                model=selected_model,
                base_url=base_url,
                use_native_tools=not args.no_tools,
            )
        except Exception as e:
            print(f"Error: Failed to get response: {str(e)}", file=sys.stderr)
            sys.exit(1)

    print(answer)
    if answer:
        history_manager.append_message({"role": "assistant", "content": answer})


if __name__ == "__main__":
    main()
